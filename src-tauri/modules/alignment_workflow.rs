// alignment_workflow.rs - å…‰æœºåˆåƒæ£€æµ‹å·¥ä½œæµç¨‹
// åŒçº¿ç¨‹æ¶æ„ï¼šé‡‡é›†çº¿ç¨‹ + å¤„ç†çº¿ç¨‹
// æ”¯æŒå®æ—¶é¢„è§ˆå’Œé˜¶æ®µåŒ–åˆåƒæ£€æµ‹

use std::sync::{Arc, Mutex, mpsc, atomic::{AtomicBool, Ordering}};
use std::thread;
use std::time::{Duration, Instant};
use std::collections::VecDeque;
use opencv::{core, imgcodecs, imgproc, prelude::*};
use tauri::{AppHandle, Emitter};
use serde::{Serialize, Deserialize};

use crate::camera_manager::{SimpleCameraManager, CameraError};
use crate::modules::{
    alignment::{AlignmentSystem, SingleEyePoseResult, DualEyeAlignmentResult, CenteringResult, AdjustmentVectors},
    param_io::*,
};

// ==================== æ•°æ®ç»“æ„å®šä¹‰ ====================

/// æ£€æµ‹é˜¶æ®µæšä¸¾ (ç®€åŒ–ç‰ˆ - ç§»é™¤WorkflowStageä¾èµ–)
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
#[serde(tag = "type")]
pub enum DetectionStage {
    Idle,                    // ç©ºé—²çŠ¶æ€
    Loading,                 // åŠ è½½å‚æ•°ä¸­
    Preview,                 // é¢„è§ˆæ¨¡å¼
    LeftEyePoseCheck,        // å·¦çœ¼å§¿æ€æ£€æµ‹
    RightEyePoseCheck,       // å³çœ¼å§¿æ€æ£€æµ‹
    DualEyeAlignment,        // åŒå…‰æœºåˆåƒæ£€æµ‹
    Completed,               // æ£€æµ‹å®Œæˆ
    Error { message: String }, // é”™è¯¯çŠ¶æ€
}

/// å¸§æ•°æ®ç»“æ„ (åŸå§‹æ•°æ®ç‰ˆæœ¬)
#[derive(Clone)]
pub struct FrameData {
    pub left_image: Vec<u8>,
    pub right_image: Vec<u8>,
    pub timestamp: Instant,
}

/// æ£€æµ‹ç»“æœ
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(tag = "stage")]
pub enum DetectionResult {
    LeftEyePose {
        roll: f64,
        pitch: f64,
        yaw: f64,
        pass: bool,
        message: String,
    },
    RightEyePose {
        roll: f64,
        pitch: f64,
        yaw: f64,
        pass: bool,
        message: String,
    },
    DualEyeAlignment {
        mean_dx: f64,
        mean_dy: f64,
        rms: f64,
        p95: f64,
        max_err: f64,
        pass: bool,
        adjustment_hint: String,
    },
    Error {
        message: String,
    },
}

/// ç¯å½¢ç¼“å†²åŒºï¼ˆä¼˜åŒ–ç‰ˆï¼‰
pub struct RingBuffer<T> {
    buffer: VecDeque<T>,
    capacity: usize,
    total_pushed: u64,
    dropped_count: u64,
}

impl<T> RingBuffer<T> {
    pub fn new(capacity: usize) -> Self {
        Self {
            buffer: VecDeque::with_capacity(capacity),
            capacity,
            total_pushed: 0,
            dropped_count: 0,
        }
    }

    pub fn push(&mut self, item: T) {
        self.total_pushed += 1;
        
        if self.buffer.len() >= self.capacity {
            self.buffer.pop_front();
            self.dropped_count += 1;
        }
        self.buffer.push_back(item);
    }

    pub fn latest(&self) -> Option<&T> {
        self.buffer.back()
    }

    pub fn len(&self) -> usize {
        self.buffer.len()
    }

    /// è·å–æ€§èƒ½ç»Ÿè®¡
    pub fn get_stats(&self) -> (u64, u64, f64) {
        let drop_rate = if self.total_pushed > 0 {
            (self.dropped_count as f64 / self.total_pushed as f64) * 100.0
        } else {
            0.0
        };
        (self.total_pushed, self.dropped_count, drop_rate)
    }
}

// ==================== ä¸»å·¥ä½œæµç¨‹ç³»ç»Ÿ ====================

pub struct AlignmentWorkflow {
    // åŸºç¡€ç»„ä»¶ (ç®€åŒ–ç‰ˆ)
    camera_manager: Arc<Mutex<SimpleCameraManager>>,
    alignment_system: Arc<Mutex<Option<AlignmentSystem>>>,
    app_handle: AppHandle,

    // çº¿ç¨‹æ§åˆ¶
    running: Arc<AtomicBool>,
    acquisition_thread: Option<thread::JoinHandle<()>>,
    processing_thread: Option<thread::JoinHandle<()>>,

    // æ•°æ®é€šä¿¡
    frame_buffer: Arc<Mutex<RingBuffer<FrameData>>>,
    stage: Arc<Mutex<DetectionStage>>,
    
    // é€šé“é€šä¿¡
    command_sender: Option<mpsc::Sender<WorkflowCommand>>,
}

/// å·¥ä½œæµç¨‹å‘½ä»¤
#[derive(Debug)]
pub enum WorkflowCommand {
    StartPreview,
    StartDetection,
    NextStage,
    Reset,
    Stop,
}

impl AlignmentWorkflow {
    /// åˆ›å»ºåˆåƒæ£€æµ‹å·¥ä½œæµç¨‹ (SimpleCameraManagerç‰ˆæœ¬)
    pub fn new(
        app_handle: AppHandle,
    ) -> Result<Self, Box<dyn std::error::Error>> {
        println!("åˆå§‹åŒ–åˆåƒæ£€æµ‹å·¥ä½œæµç¨‹ (SimpleCameraManagerç‰ˆæœ¬)...");

        // åˆ›å»ºSimpleCameraManager
        let camera_manager = Arc::new(Mutex::new(SimpleCameraManager::new()?));
        let frame_buffer = Arc::new(Mutex::new(RingBuffer::new(5))); // ä¿æŒæœ€è¿‘5å¸§
        let stage = Arc::new(Mutex::new(DetectionStage::Idle));

        Ok(Self {
            camera_manager,
            alignment_system: Arc::new(Mutex::new(None)),
            app_handle,
            running: Arc::new(AtomicBool::new(false)),
            acquisition_thread: None,
            processing_thread: None,
            frame_buffer,
            stage,
            command_sender: None,
        })
    }

    /// åˆå§‹åŒ–åˆåƒæ£€æµ‹ç³»ç»Ÿï¼ˆåŠ è½½å‚æ•°ï¼‰
    pub fn initialize_alignment_system(&mut self) -> Result<(), Box<dyn std::error::Error>> {
        println!("=== åˆå§‹åŒ–åˆåƒæ£€æµ‹ç³»ç»Ÿ ===");
        
        // æ›´æ–°çŠ¶æ€
        *self.stage.lock().unwrap() = DetectionStage::Loading;
        self.emit_stage_update()?;

        // åŠ è½½æ ‡å®šå‚æ•°
        let image_size = core::Size::new(2448, 2048);
        
        // ğŸ”§ ä¿®æ­£å‚æ•°æ–‡ä»¶è·¯å¾„ - ä½¿ç”¨yaml_last_param_fileç›®å½•
        // æ—§è·¯å¾„ (æ³¨é‡Šæ‰):
        // "left_camera_params.yaml",
        // "right_camera_params.yaml", 
        // "stereo_params.yaml",
        // "rectify_params.yaml",
        
        let alignment_sys = AlignmentSystem::new(
            image_size,
            "yaml_last_param_file/left_camera_params.yaml",
            "yaml_last_param_file/right_camera_params.yaml", 
            "yaml_last_param_file/stereo_params.yaml",
            "yaml_last_param_file/rectify_params.yaml",
        )?;

        *self.alignment_system.lock().unwrap() = Some(alignment_sys);
        
        println!("âœ“ åˆåƒæ£€æµ‹ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ");
        *self.stage.lock().unwrap() = DetectionStage::Idle;
        self.emit_stage_update()?;
        
        Ok(())
    }

    /// å¯åŠ¨å·¥ä½œæµç¨‹ï¼ˆåŒçº¿ç¨‹æ¨¡å¼ - SimpleCameraManagerç‰ˆæœ¬ï¼‰
    pub fn start_workflow(&mut self) -> Result<(), Box<dyn std::error::Error>> {
        println!("ğŸš€ å¯åŠ¨åˆåƒæ£€æµ‹å·¥ä½œæµç¨‹...");
        
        if self.running.load(Ordering::SeqCst) {
            return Err("å·¥ä½œæµç¨‹å·²ç»åœ¨è¿è¡Œä¸­".into());
        }
        
        // [é…ç½®ç³»ç»Ÿ - å·²æ³¨é‡Š] è®¾ç½®ç›¸æœºä¸ºåˆåƒæ£€æµ‹æ¨¡å¼
        // unsafe {
        //     crate::camera_ffi::set_camera_mode(2); // 2 = alignment mode
        // }
        // println!("ğŸ“· å·²è®¾ç½®ç›¸æœºä¸ºåˆåƒæ£€æµ‹æ¨¡å¼");
        
        // åˆ›å»ºSimpleCameraManagerå¹¶å¯åŠ¨
        {
            let mut cam = self.camera_manager.lock()
                .map_err(|e| format!("è·å–ç›¸æœºç®¡ç†å™¨å¤±è´¥: {}", e))?;
            cam.start()
                .map_err(|e| format!("å¯åŠ¨ç›¸æœºå¤±è´¥: {:?}", e))?;
        }

        // åˆå§‹åŒ–ç³»ç»Ÿï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
        if self.alignment_system.lock().unwrap().is_none() {
            self.initialize_alignment_system()?;
        }

        // ä¼˜åŒ–OpenCVæ€§èƒ½é…ç½®
        self.configure_opencv_performance()?;

        self.running.store(true, Ordering::SeqCst);
        
        // åˆ›å»ºå‘½ä»¤é€šé“
        let (cmd_tx, cmd_rx) = mpsc::channel();
        self.command_sender = Some(cmd_tx);

        // å¯åŠ¨é‡‡é›†çº¿ç¨‹
        self.start_acquisition_thread()?;
        
        // å¯åŠ¨å¤„ç†çº¿ç¨‹
        self.start_processing_thread(cmd_rx)?;

        // å¯åŠ¨é¢„è§ˆæ¨¡å¼
        self.send_command(WorkflowCommand::StartPreview)?;

        println!("âœ“ å·¥ä½œæµç¨‹å¯åŠ¨å®Œæˆ");
        Ok(())
    }

    /// é…ç½®OpenCVæ€§èƒ½ä¼˜åŒ–
    fn configure_opencv_performance(&self) -> Result<(), Box<dyn std::error::Error>> {
        // è®¾ç½®OpenCVçº¿ç¨‹æ•°ä¸ºCPUæ ¸å¿ƒæ•°çš„ä¸€åŠï¼Œé¿å…è¿‡åº¦å¹¶è¡Œ
        let cpu_cores = num_cpus::get();
        let opencv_threads = (cpu_cores / 2).max(1).min(4); // é™åˆ¶åœ¨1-4ä¹‹é—´
        
        #[cfg(feature = "opencv")]
        {
            opencv::core::set_num_threads(opencv_threads as i32)
                .map_err(|e| format!("è®¾ç½®OpenCVçº¿ç¨‹æ•°å¤±è´¥: {}", e))?;
            println!("ğŸ”§ OpenCVçº¿ç¨‹æ•°è®¾ç½®ä¸º: {} (CPUæ ¸å¿ƒ: {})", opencv_threads, cpu_cores);
        }

        // å¯ç”¨OpenCVä¼˜åŒ–
        #[cfg(feature = "opencv")]
        {
            opencv::core::set_use_optimized(true)
                .map_err(|e| format!("å¯ç”¨OpenCVä¼˜åŒ–å¤±è´¥: {}", e))?;
            println!("ğŸš€ OpenCVä¼˜åŒ–å·²å¯ç”¨");
        }

        Ok(())
    }

    /// å¯åŠ¨é‡‡é›†çº¿ç¨‹ (SimpleCameraManagerç‰ˆæœ¬)
    fn start_acquisition_thread(&mut self) -> Result<(), Box<dyn std::error::Error>> {
        let camera_manager = Arc::clone(&self.camera_manager);
        let frame_buffer = Arc::clone(&self.frame_buffer);
        let running = Arc::clone(&self.running);

        let handle = thread::spawn(move || {
            println!("ğŸ“· é‡‡é›†çº¿ç¨‹å¯åŠ¨ (SimpleCameraManagerç‰ˆæœ¬)");
            
            // ç›¸æœºå·²ç»åœ¨ start_workflow() ä¸­å¯åŠ¨ï¼Œè¿™é‡Œä¸éœ€è¦é‡å¤å¯åŠ¨
            // ç§»é™¤é‡å¤çš„å¯åŠ¨ä»£ç ï¼š
            // if let Err(e) = camera_manager.lock().unwrap().start() {
            //     eprintln!("ç›¸æœºå¯åŠ¨å¤±è´¥: {:?}", e);
            //     return;
            // }

            let mut frame_count = 0u64;
            let mut last_stats_time = Instant::now();

            // 10fps = 100msé—´éš”
            let frame_interval = Duration::from_millis(100);
            let mut last_capture_time = Instant::now();

            while running.load(Ordering::SeqCst) {
                let now = Instant::now();
                
                // æ§åˆ¶å¸§ç‡
                if now.duration_since(last_capture_time) >= frame_interval {
                    match camera_manager.lock().unwrap().get_current_frame() {
                        Ok((left_data, right_data)) => {
                            let frame = FrameData {
                                left_image: left_data,
                                right_image: right_data,
                                timestamp: now,
                            };

                            // æ¨å…¥ç¯å½¢ç¼“å†²åŒº
                            frame_buffer.lock().unwrap().push(frame);
                            frame_count += 1;
                            last_capture_time = now;
                        }
                        Err(e) => {
                            eprintln!("é‡‡é›†å¸§å¤±è´¥: {:?}", e);
                            // æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢
                            if !running.load(Ordering::SeqCst) {
                                break;
                            }
                            thread::sleep(Duration::from_millis(50));
                        }
                    }
                }

                // ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ¯5ç§’è¾“å‡ºä¸€æ¬¡ï¼‰
                if now.duration_since(last_stats_time) >= Duration::from_secs(5) {
                    println!("ğŸ“Š é‡‡é›†ç»Ÿè®¡: {}å¸§, ç¼“å†²åŒº: {}å¸§", 
                             frame_count, frame_buffer.lock().unwrap().len());
                    last_stats_time = now;
                }

                // æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢
                if !running.load(Ordering::SeqCst) {
                    break;
                }
                
                thread::sleep(Duration::from_millis(10));
            }

            // åœæ­¢ç›¸æœº
            let _ = camera_manager.lock().unwrap().stop();
            println!("ğŸ“· é‡‡é›†çº¿ç¨‹ç»“æŸ");
        });

        self.acquisition_thread = Some(handle);
        Ok(())
    }

    /// å¯åŠ¨å¤„ç†çº¿ç¨‹
    fn start_processing_thread(
        &mut self,
        cmd_rx: mpsc::Receiver<WorkflowCommand>,
    ) -> Result<(), Box<dyn std::error::Error>> {
        let frame_buffer = Arc::clone(&self.frame_buffer);
        let stage = Arc::clone(&self.stage);
        let alignment_system = Arc::clone(&self.alignment_system);
        let running = Arc::clone(&self.running);
        let app_handle = self.app_handle.clone();

        let handle = thread::spawn(move || {
            println!("ğŸ”„ å¤„ç†çº¿ç¨‹å¯åŠ¨");

            while running.load(Ordering::SeqCst) {
                // å¤„ç†å‘½ä»¤
                if let Ok(cmd) = cmd_rx.try_recv() {
                    match cmd {
                        WorkflowCommand::StartPreview => {
                            *stage.lock().unwrap() = DetectionStage::Preview;
                            let _ = app_handle.emit("alignment-stage", DetectionStage::Preview);
                        }
                        WorkflowCommand::StartDetection => {
                            *stage.lock().unwrap() = DetectionStage::LeftEyePoseCheck;
                            let _ = app_handle.emit("alignment-stage", DetectionStage::LeftEyePoseCheck);
                        }
                        WorkflowCommand::NextStage => {
                            // å¤„ç†é˜¶æ®µè½¬æ¢é€»è¾‘
                            Self::handle_stage_transition(&stage, &app_handle);
                        }
                        WorkflowCommand::Reset => {
                            *stage.lock().unwrap() = DetectionStage::Preview;
                            let _ = app_handle.emit("alignment-stage", DetectionStage::Preview);
                        }
                        WorkflowCommand::Stop => {
                            running.store(false, Ordering::SeqCst);
                            break;
                        }
                    }
                }

                // æ ¹æ®å½“å‰é˜¶æ®µå¤„ç†å›¾åƒ
                let current_stage = stage.lock().unwrap().clone();
                match current_stage {
                    DetectionStage::Preview => {
                        // é¢„è§ˆæ¨¡å¼ï¼šå®šæœŸå‘é€é¢„è§ˆå›¾åƒ
                        Self::handle_preview_mode(&frame_buffer, &app_handle);
                    }
                    DetectionStage::LeftEyePoseCheck |
                    DetectionStage::RightEyePoseCheck |
                    DetectionStage::DualEyeAlignment => {
                        // æ£€æµ‹æ¨¡å¼ï¼šå¤„ç†æœ€æ–°å¸§
                        Self::handle_detection_mode(
                            &frame_buffer,
                            &alignment_system,
                            &current_stage,
                            &app_handle,
                        );
                    }
                    _ => {}
                }

                thread::sleep(Duration::from_millis(50));
            }

            println!("ğŸ”„ å¤„ç†çº¿ç¨‹ç»“æŸ");
        });

        self.processing_thread = Some(handle);
        Ok(())
    }

    /// å¤„ç†é¢„è§ˆæ¨¡å¼ (åŸå§‹æ•°æ®ç‰ˆæœ¬)
    fn handle_preview_mode(
        frame_buffer: &Arc<Mutex<RingBuffer<FrameData>>>,
        app_handle: &AppHandle,
    ) {
        if let Some(frame) = frame_buffer.lock().unwrap().latest() {
            // æ¯200mså‘é€ä¸€æ¬¡é¢„è§ˆå›¾åƒï¼ˆ5fpsé¢„è§ˆï¼‰
            // æ³¨æ„ï¼šè¿™é‡Œå‘é€åŸå§‹æ•°æ®ï¼Œå‰ç«¯éœ€è¦ç›¸åº”å¤„ç†
            let preview_data = serde_json::json!({
                "left_preview_size": frame.left_image.len(),
                "right_preview_size": frame.right_image.len(),
                "timestamp": frame.timestamp.elapsed().as_millis(),
                "width": 2448,
                "height": 2048,
                "format": "grayscale"
            });
            
            let _ = app_handle.emit("alignment-preview", preview_data);
        }
        
        thread::sleep(Duration::from_millis(200));
    }

    /// å¤„ç†æ£€æµ‹æ¨¡å¼
    fn handle_detection_mode(
        frame_buffer: &Arc<Mutex<RingBuffer<FrameData>>>,
        alignment_system: &Arc<Mutex<Option<AlignmentSystem>>>,
        stage: &DetectionStage,
        app_handle: &AppHandle,
    ) {
        let start_time = Instant::now();
        
        let frame = {
            let buffer = frame_buffer.lock().unwrap();
            buffer.latest().cloned()
        };

        if let Some(frame_data) = frame {
            let mut alignment_sys = alignment_system.lock().unwrap();
            if let Some(ref mut sys) = *alignment_sys {
                match Self::process_detection_frame(sys, &frame_data, stage) {
                    Ok(result) => {
                        let processing_time = start_time.elapsed();
                        println!("ğŸ” æ£€æµ‹å¤„ç†è€—æ—¶: {:.1}ms", processing_time.as_millis());
                        
                        let _ = app_handle.emit("alignment-result", result);
                    }
                    Err(e) => {
                        let error_result = DetectionResult::Error {
                            message: format!("æ£€æµ‹å¤„ç†å¤±è´¥: {}", e),
                        };
                        let _ = app_handle.emit("alignment-result", error_result);
                    }
                }
            }
        }

        // æ£€æµ‹æ¨¡å¼ä¸‹é™ä½å¤„ç†é¢‘ç‡ï¼Œé¿å…CPUè¿‡è½½
        thread::sleep(Duration::from_millis(200));
    }

    /// å¤„ç†æ£€æµ‹å¸§ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
    fn process_detection_frame(
        alignment_sys: &mut AlignmentSystem,
        frame_data: &FrameData,
        stage: &DetectionStage,
    ) -> Result<DetectionResult, Box<dyn std::error::Error>> {
        // å°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºOpenCV Mat
        let left_image = Self::raw_data_to_mat(&frame_data.left_image, 2448, 2048)?;
        let right_image = Self::raw_data_to_mat(&frame_data.right_image, 2448, 2048)?;

        // æ ¹æ®æ£€æµ‹é˜¶æ®µä¼˜åŒ–å¤„ç†ç­–ç•¥
        match stage {
            DetectionStage::LeftEyePoseCheck => {
                // åªæ£€æµ‹å·¦çœ¼åœ†å¿ƒï¼Œæé«˜æ•ˆç‡
                let (corners_left, _) = alignment_sys.detect_circles_grid(
                    &left_image,
                    &right_image, // ä»éœ€ä¼ å…¥ï¼Œä½†å†…éƒ¨å¯ä»¥ä¼˜åŒ–åªå¤„ç†å·¦çœ¼
                    "yaml_last_param_file/rectify_maps.yaml", // ğŸ”§ ä¿®æ­£è·¯å¾„
                )?;
                
                // ä½¿ç”¨å‘åå…¼å®¹çš„å·¦çœ¼å§¿æ€æ£€æµ‹æ–¹æ³•
                let result = alignment_sys.check_left_eye_pose(&corners_left)?;
                Ok(DetectionResult::LeftEyePose {
                    roll: result.roll,
                    pitch: result.pitch,
                    yaw: result.yaw,
                    pass: result.pass,
                    message: if result.pass {
                        "âœ“ å·¦çœ¼å§¿æ€æ£€æµ‹é€šè¿‡".to_string()
                    } else {
                        format!("âŒ å·¦çœ¼å§¿æ€è¶…å‡ºå®¹å·® - roll={:.3}Â°, pitch={:.3}Â°, yaw={:.3}Â°", 
                               result.roll, result.pitch, result.yaw)
                    },
                })
            }
            DetectionStage::RightEyePoseCheck => {
                // åªæ£€æµ‹å³çœ¼åœ†å¿ƒ
                let (_, corners_right) = alignment_sys.detect_circles_grid(
                    &left_image,
                    &right_image,
                    "yaml_last_param_file/rectify_maps.yaml", // ğŸ”§ ä¿®æ­£è·¯å¾„
                )?;
                
                // ä½¿ç”¨å‘åå…¼å®¹çš„å³çœ¼å§¿æ€æ£€æµ‹æ–¹æ³•
                let result = alignment_sys.check_right_eye_pose(&corners_right)?;
                Ok(DetectionResult::RightEyePose {
                    roll: result.roll,
                    pitch: result.pitch,
                    yaw: result.yaw,
                    pass: result.pass,
                    message: if result.pass {
                        "âœ“ å³çœ¼å§¿æ€æ£€æµ‹é€šè¿‡".to_string()
                    } else {
                        format!("âŒ å³çœ¼å§¿æ€è¶…å‡ºå®¹å·® - roll={:.3}Â°, pitch={:.3}Â°, yaw={:.3}Â°", 
                               result.roll, result.pitch, result.yaw)
                    },
                })
            }
            DetectionStage::DualEyeAlignment => {
                // åŒçœ¼åŒæ—¶æ£€æµ‹ï¼Œæœ€é«˜ç²¾åº¦
                let (corners_left, corners_right) = alignment_sys.detect_circles_grid(
                    &left_image,
                    &right_image,
                    "yaml_last_param_file/rectify_maps.yaml", // ğŸ”§ ä¿®æ­£è·¯å¾„
                )?;
                
                let result = alignment_sys.check_dual_eye_alignment(&corners_left, &corners_right, true)?;
                let adjustment_hint = format!(
                    "è°ƒæ•´æç¤º: Î”x={:.3}px {}, Î”y={:.3}px {}",
                    result.mean_dx,
                    if result.mean_dx > 0.0 { "(å³çœ¼å‘å·¦è°ƒ)" } else { "(å³çœ¼å‘å³è°ƒ)" },
                    result.mean_dy,
                    if result.mean_dy < 0.0 { "(å³çœ¼å‘ä¸Šè°ƒ)" } else { "(å³çœ¼å‘ä¸‹è°ƒ)" }
                );

                Ok(DetectionResult::DualEyeAlignment {
                    mean_dx: result.mean_dx,
                    mean_dy: result.mean_dy,
                    rms: result.rms,
                    p95: result.p95,
                    max_err: result.max_err,
                    pass: result.pass,
                    adjustment_hint,
                })
            }
            _ => Err("ä¸æ”¯æŒçš„æ£€æµ‹é˜¶æ®µ".into()),
        }
    }

    /// å°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºOpenCV Mat
    fn raw_data_to_mat(data: &[u8], width: i32, height: i32) -> Result<core::Mat, opencv::Error> {
        // åˆ›å»ºç©ºçš„Mat
        let mut mat = core::Mat::new_rows_cols_with_default(
            height,
            width,
            core::CV_8UC1,
            core::Scalar::default(),
        )?;
        
        // å°†æ•°æ®æ‹·è´åˆ°Matä¸­
        let mat_data = mat.data_mut();
        let expected_size = (width * height) as usize;
        
        if data.len() >= expected_size {
            unsafe {
                std::ptr::copy_nonoverlapping(
                    data.as_ptr(),
                    mat_data,
                    expected_size,
                );
            }
        } else {
            return Err(opencv::Error::new(
                opencv::core::StsError, 
                format!("æ•°æ®é•¿åº¦ä¸è¶³: éœ€è¦{}å­—èŠ‚ï¼Œå®é™…{}å­—èŠ‚", expected_size, data.len())
            ));
        }
        
        Ok(mat)
    }

    /// å¤„ç†é˜¶æ®µè½¬æ¢
    fn handle_stage_transition(
        stage: &Arc<Mutex<DetectionStage>>,
        app_handle: &AppHandle,
    ) {
        let mut current_stage = stage.lock().unwrap();
        let next_stage = match *current_stage {
            DetectionStage::LeftEyePoseCheck => DetectionStage::RightEyePoseCheck,
            DetectionStage::RightEyePoseCheck => DetectionStage::DualEyeAlignment,
            DetectionStage::DualEyeAlignment => DetectionStage::Completed,
            _ => return,
        };

        *current_stage = next_stage.clone();
        let _ = app_handle.emit("alignment-stage", next_stage);
    }

    // ==================== å…¬å…±æ¥å£æ–¹æ³• ====================

    /// å‘é€å‘½ä»¤
    pub fn send_command(&self, cmd: WorkflowCommand) -> Result<(), Box<dyn std::error::Error>> {
        if let Some(ref sender) = self.command_sender {
            sender.send(cmd)?;
        }
        Ok(())
    }

    /// å¼€å§‹æ£€æµ‹
    pub fn start_detection(&self) -> Result<(), Box<dyn std::error::Error>> {
        self.send_command(WorkflowCommand::StartDetection)
    }

    /// ä¸‹ä¸€é˜¶æ®µ
    pub fn next_stage(&self) -> Result<(), Box<dyn std::error::Error>> {
        self.send_command(WorkflowCommand::NextStage)
    }

    /// é‡ç½®åˆ°é¢„è§ˆæ¨¡å¼
    pub fn reset_to_preview(&self) -> Result<(), Box<dyn std::error::Error>> {
        self.send_command(WorkflowCommand::Reset)
    }

    /// åœæ­¢å·¥ä½œæµç¨‹
    pub fn stop_workflow(&mut self) -> Result<(), Box<dyn std::error::Error>> {
        if !self.running.load(Ordering::SeqCst) {
            return Ok(());
        }

        println!("=== åœæ­¢åˆåƒæ£€æµ‹å·¥ä½œæµç¨‹ ===");
        
        // ç«‹å³è®¾ç½®åœæ­¢æ ‡å¿—
        self.running.store(false, Ordering::SeqCst);
        
        // å‘é€åœæ­¢å‘½ä»¤
        if let Some(ref sender) = self.command_sender {
            let _ = sender.send(WorkflowCommand::Stop);
        }
        
        // å¼ºåˆ¶åœæ­¢ç›¸æœºï¼ˆå¦‚æœçº¿ç¨‹æ²¡æœ‰åŠæ—¶å“åº”ï¼‰
        if let Ok(camera_manager) = self.camera_manager.lock() {
            let _ = camera_manager.stop();
            println!("ğŸ›‘ å¼ºåˆ¶åœæ­¢ç›¸æœº");
        }
        
        // ç­‰å¾…çº¿ç¨‹ç»“æŸï¼ˆè®¾ç½®è¶…æ—¶ï¼‰
        if let Some(handle) = self.acquisition_thread.take() {
            println!("â³ ç­‰å¾…é‡‡é›†çº¿ç¨‹ç»“æŸ...");
            match handle.join() {
                Ok(_) => println!("âœ“ é‡‡é›†çº¿ç¨‹å·²ç»“æŸ"),
                Err(e) => println!("âš ï¸ é‡‡é›†çº¿ç¨‹ç»“æŸå¼‚å¸¸: {:?}", e),
            }
        }
        
        if let Some(handle) = self.processing_thread.take() {
            println!("â³ ç­‰å¾…å¤„ç†çº¿ç¨‹ç»“æŸ...");
            match handle.join() {
                Ok(_) => println!("âœ“ å¤„ç†çº¿ç¨‹å·²ç»“æŸ"),
                Err(e) => println!("âš ï¸ å¤„ç†çº¿ç¨‹ç»“æŸå¼‚å¸¸: {:?}", e),
            }
        }

        println!("âœ“ å·¥ä½œæµç¨‹å·²åœæ­¢");
        Ok(())
    }

    /// è·å–å½“å‰çŠ¶æ€
    pub fn get_current_stage(&self) -> DetectionStage {
        self.stage.lock().unwrap().clone()
    }

    /// å‘é€çŠ¶æ€æ›´æ–°äº‹ä»¶
    fn emit_stage_update(&self) -> Result<(), Box<dyn std::error::Error>> {
        let stage = self.get_current_stage();
        self.app_handle.emit("alignment-stage", stage)?;
        Ok(())
    }

    /// è·å–å½“å‰é¢„è§ˆå¸§ï¼ˆBase64æ ¼å¼ï¼‰
    pub fn get_current_preview_frame(&self) -> Result<crate::commands::alignment_commands::CameraPreviewData, Box<dyn std::error::Error>> {
        use base64::{Engine as _, engine::general_purpose};
        
        // ä»ç¼“å†²åŒºè·å–æœ€æ–°å¸§
        let frame_data = {
            let buffer = self.frame_buffer.lock().unwrap();
            buffer.latest().cloned()
        };
        
        if let Some(frame) = frame_data {
            // // ===== DEBUG START: å¯åœ¨æ­£å¼ç‰ˆæœ¬ä¸­åˆ é™¤ =====
            // // ğŸ” DEBUG: ä¿å­˜åŸå§‹å›¾åƒç”¨äºè°ƒè¯•ï¼ˆæ¯100æ¬¡è°ƒç”¨ä¿å­˜ä¸€æ¬¡ï¼‰
            // static mut DEBUG_COUNTER: u32 = 0;
            // unsafe {
            //     DEBUG_COUNTER += 1;
            //     if DEBUG_COUNTER % 100 == 0 {
            //         self.save_debug_images(&frame)?;
            //     }
            // }
            // // ===== DEBUG END: å¯åœ¨æ­£å¼ç‰ˆæœ¬ä¸­åˆ é™¤ =====
            
            // å°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºBase64å›¾åƒ
            let left_base64 = raw_data_to_base64_image(&frame.left_image, 2448, 2048)?;
            let right_base64 = raw_data_to_base64_image(&frame.right_image, 2448, 2048)?;
            
            Ok(crate::commands::alignment_commands::CameraPreviewData {
                left_image_base64: left_base64,
                right_image_base64: right_base64,
                timestamp: frame.timestamp.elapsed().as_millis() as u64,
                width: 2448,
                height: 2048,
                fps: 10.0,
            })
        } else {
            Err("æ²¡æœ‰å¯ç”¨çš„å¸§æ•°æ®".into())
        }
    }

    /// è·å–å½“å‰æ£€æµ‹ç»“æœ
    pub fn get_current_detection_result(&self) -> Result<DetectionResult, Box<dyn std::error::Error>> {
        // ä»ç¼“å†²åŒºè·å–æœ€æ–°å¸§
        let frame_data = {
            let buffer = self.frame_buffer.lock().unwrap();
            buffer.latest().cloned()
        };
        
        if let Some(frame) = frame_data {
            let mut alignment_sys = self.alignment_system.lock().unwrap();
            if let Some(ref mut sys) = *alignment_sys {
                // æ‰§è¡Œå®Œæ•´çš„æ£€æµ‹æµç¨‹
                let left_image = Self::raw_data_to_mat(&frame.left_image, 2448, 2048)?;
                let right_image = Self::raw_data_to_mat(&frame.right_image, 2448, 2048)?;
                
                // ä½¿ç”¨å•å¸§æ£€æµ‹æ–¹æ³•
                self.detect_single_frame_internal(sys, left_image, right_image)
            } else {
                Err("åˆåƒæ£€æµ‹ç³»ç»Ÿæœªåˆå§‹åŒ–".into())
            }
        } else {
            Err("æ²¡æœ‰å¯ç”¨çš„å¸§æ•°æ®".into())
        }
    }
    
    /// å†…éƒ¨å•å¸§æ£€æµ‹æ–¹æ³•
    fn detect_single_frame_internal(
        &self,
        alignment_sys: &mut crate::modules::alignment::AlignmentSystem,
        left_image: opencv::core::Mat,
        right_image: opencv::core::Mat,
    ) -> Result<DetectionResult, Box<dyn std::error::Error>> {
        // 1. æ‰§è¡Œåœ†å¿ƒæ£€æµ‹
        let (left_corners, right_corners) = alignment_sys.detect_circles_grid(
            &left_image,
            &right_image,
            "yaml_last_param_file/rectify_maps.yaml", // ğŸ”§ ä¿®æ­£è·¯å¾„
        )?;
        
        // 2. å·¦çœ¼å§¿æ€æ£€æµ‹
        let left_pose = alignment_sys.check_left_eye_pose(&left_corners)?;
        if !left_pose.pass {
            return Ok(DetectionResult::LeftEyePose {
                roll: left_pose.roll,
                pitch: left_pose.pitch,
                yaw: left_pose.yaw,
                pass: false,
                message: format!("âŒ å·¦çœ¼å§¿æ€è¶…å‡ºå®¹å·® - roll={:.3}Â°, pitch={:.3}Â°, yaw={:.3}Â°", 
                               left_pose.roll, left_pose.pitch, left_pose.yaw),
            });
        }
        
        // 3. å³çœ¼å§¿æ€æ£€æµ‹
        let right_pose = alignment_sys.check_right_eye_pose(&right_corners)?;
        if !right_pose.pass {
            return Ok(DetectionResult::RightEyePose {
                roll: right_pose.roll,
                pitch: right_pose.pitch,
                yaw: right_pose.yaw,
                pass: false,
                message: format!("âŒ å³çœ¼å§¿æ€è¶…å‡ºå®¹å·® - roll={:.3}Â°, pitch={:.3}Â°, yaw={:.3}Â°", 
                               right_pose.roll, right_pose.pitch, right_pose.yaw),
            });
        }
        
        // 4. åŒçœ¼åˆåƒæ£€æµ‹
        let alignment_result = alignment_sys.check_dual_eye_alignment(&left_corners, &right_corners, false)?;
        let adjustment_hint = format!(
            "è°ƒæ•´æç¤º: Î”x={:.3}px {}, Î”y={:.3}px {}",
            alignment_result.mean_dx,
            if alignment_result.mean_dx > 0.0 { "(å³çœ¼å‘å·¦è°ƒ)" } else { "(å³çœ¼å‘å³è°ƒ)" },
            alignment_result.mean_dy,
            if alignment_result.mean_dy < 0.0 { "(å³çœ¼å‘ä¸Šè°ƒ)" } else { "(å³çœ¼å‘ä¸‹è°ƒ)" }
        );
        
        Ok(DetectionResult::DualEyeAlignment {
            mean_dx: alignment_result.mean_dx,
            mean_dy: alignment_result.mean_dy,
            rms: alignment_result.rms,
            p95: alignment_result.p95,
            max_err: alignment_result.max_err,
            pass: alignment_result.pass,
            adjustment_hint,
        })
    }

    /// è·å–ç³»ç»Ÿæ€§èƒ½ç»Ÿè®¡
    pub fn get_performance_stats(&self) -> Result<serde_json::Value, Box<dyn std::error::Error>> {
        let buffer_stats = {
            let buffer = self.frame_buffer.lock().unwrap();
            buffer.get_stats()
        };

        let stats = serde_json::json!({
            "buffer": {
                "total_frames": buffer_stats.0,
                "dropped_frames": buffer_stats.1,
                "drop_rate_percent": buffer_stats.2,
                "current_size": self.frame_buffer.lock().unwrap().len(),
                "capacity": 5
            },
            "system": {
                "cpu_cores": num_cpus::get(),
                "opencv_threads": 2, // å·²åœ¨configure_opencv_performanceä¸­è®¾ç½®
                "thread_count": 2,   // é‡‡é›†çº¿ç¨‹ + å¤„ç†çº¿ç¨‹
                "running": self.running.load(Ordering::SeqCst)
            },
            "stage": self.get_current_stage()
        });

        Ok(stats)
    }

    /// æ‰‹åŠ¨ä¿å­˜è°ƒè¯•å›¾åƒï¼ˆå…¬å¼€æ¥å£ï¼‰
    pub fn save_debug_images_manual(&self) -> Result<(), Box<dyn std::error::Error>> {
        let frame_data = {
            let buffer = self.frame_buffer.lock().unwrap();
            buffer.latest().cloned()
        };
        
        if let Some(frame) = frame_data {
            self.save_debug_images(&frame)
        } else {
            Err("æ²¡æœ‰å¯ç”¨çš„å¸§æ•°æ®".into())
        }
    }
    
    // ===== DEBUG START: å¯åœ¨æ­£å¼ç‰ˆæœ¬ä¸­åˆ é™¤ =====
    /// ğŸ” DEBUG: ä¿å­˜è°ƒè¯•å›¾åƒ
    fn save_debug_images(&self, frame: &FrameData) -> Result<(), Box<dyn std::error::Error>> {
        use opencv::{imgcodecs, core::Vector};
        use std::time::SystemTime;
        
        println!("ğŸ“¸ ä¿å­˜è°ƒè¯•å›¾åƒ...");
        
        // è½¬æ¢ä¸ºMatæ ¼å¼
        let left_mat = Self::raw_data_to_mat(&frame.left_image, 2448, 2048)?;
        let right_mat = Self::raw_data_to_mat(&frame.right_image, 2448, 2048)?;
        
        // ç”Ÿæˆæ—¶é—´æˆ³æ–‡ä»¶å
        let timestamp = SystemTime::now()
            .duration_since(SystemTime::UNIX_EPOCH)
            .unwrap()
            .as_secs();
        
        // ç¡®ä¿è°ƒè¯•ç›®å½•å­˜åœ¨
        let debug_dir = "src-tauri/captures/alignment_workflow_debug";
        std::fs::create_dir_all(debug_dir)?;
        
        let left_path = format!("{}/debug_left_{}.png", debug_dir, timestamp);
        let right_path = format!("{}/debug_right_{}.png", debug_dir, timestamp);
        
        // ä¿å­˜åŸå§‹å›¾åƒ
        imgcodecs::imwrite(&left_path, &left_mat, &Vector::new())?;
        imgcodecs::imwrite(&right_path, &right_mat, &Vector::new())?;
        println!("âœ… å·²ä¿å­˜è°ƒè¯•å›¾åƒ: {} å’Œ {}", left_path, right_path);
        
        // å¦‚æœalignment_systemå·²åˆå§‹åŒ–ï¼Œä¹Ÿä¿å­˜é‡æ˜ å°„åçš„å›¾åƒ
        if let Ok(alignment_sys) = self.alignment_system.lock() {
            if let Some(sys) = alignment_sys.as_ref() {
                // ç¡®ä¿é‡æ˜ å°„çŸ©é˜µå·²åŠ è½½
                if sys.get_rectify_maps().is_some() {
                    println!("ğŸ“¸ ä¿å­˜é‡æ˜ å°„åçš„å›¾åƒ...");
                    
                    // æ‰§è¡Œé‡æ˜ å°„
                    let (left_map1, left_map2, right_map1, right_map2) = sys.get_rectify_maps().unwrap();
                    let rectifier = sys.get_rectifier();
                    
                    let left_rect = rectifier.remap_image_adaptive(&left_mat, left_map1, left_map2)?;
                    let right_rect = rectifier.remap_image_adaptive(&right_mat, right_map1, right_map2)?;
                    
                    let left_rect_path = format!("{}/debug_left_rectified_{}.png", debug_dir, timestamp);
                    let right_rect_path = format!("{}/debug_right_rectified_{}.png", debug_dir, timestamp);
                    
                    imgcodecs::imwrite(&left_rect_path, &left_rect, &Vector::new())?;
                    imgcodecs::imwrite(&right_rect_path, &right_rect, &Vector::new())?;
                    println!("âœ… å·²ä¿å­˜é‡æ˜ å°„å›¾åƒ: {} å’Œ {}", left_rect_path, right_rect_path);
                }
            }
        }
        
        Ok(())
    }
    // ===== DEBUG END: å¯åœ¨æ­£å¼ç‰ˆæœ¬ä¸­åˆ é™¤ =====
    
    /// æ‰“å°æ€§èƒ½æŠ¥å‘Š
    pub fn print_performance_report(&self) {
        if let Ok(stats) = self.get_performance_stats() {
            println!("ğŸ“Š === æ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š ===");
            if let Some(buffer) = stats.get("buffer") {
                println!("ğŸ—‚ï¸  ç¼“å†²åŒºç»Ÿè®¡:");
                println!("   æ€»å¸§æ•°: {}", buffer.get("total_frames").unwrap_or(&serde_json::Value::Null));
                println!("   ä¸¢å¸§æ•°: {}", buffer.get("dropped_frames").unwrap_or(&serde_json::Value::Null));
                println!("   ä¸¢å¸§ç‡: {:.2}%", buffer.get("drop_rate_percent").unwrap_or(&serde_json::Value::Null));
                println!("   å½“å‰å¤§å°: {}/{}", 
                    buffer.get("current_size").unwrap_or(&serde_json::Value::Null),
                    buffer.get("capacity").unwrap_or(&serde_json::Value::Null));
            }
            if let Some(system) = stats.get("system") {
                println!("ğŸ’» ç³»ç»Ÿé…ç½®:");
                println!("   CPUæ ¸å¿ƒ: {}", system.get("cpu_cores").unwrap_or(&serde_json::Value::Null));
                println!("   OpenCVçº¿ç¨‹: {}", system.get("opencv_threads").unwrap_or(&serde_json::Value::Null));
                println!("   å·¥ä½œçº¿ç¨‹: {}", system.get("thread_count").unwrap_or(&serde_json::Value::Null));
                println!("   è¿è¡ŒçŠ¶æ€: {}", system.get("running").unwrap_or(&serde_json::Value::Null));
            }
            println!("========================");
        }
    }
    
    /// ğŸ¯ å•å¸§æ£€æµ‹æ–¹æ³• - é€‚åˆå‰ç«¯Tauriå‘½ä»¤è°ƒç”¨
    /// 
    /// è¿™ä¸ªæ–¹æ³•å°è£…äº†å®Œæ•´çš„æ£€æµ‹æµç¨‹ï¼šåœ†å¿ƒæ£€æµ‹ -> å§¿æ€åˆ†æ -> åˆåƒæ£€æµ‹
    /// é€‚åˆå‰ç«¯æŒ‰é’®è§¦å‘çš„å•æ¬¡æ£€æµ‹æ“ä½œ
    pub fn detect_single_frame(
        &mut self,
        left_image: core::Mat,
        right_image: core::Mat,
    ) -> Result<DetectionResult, Box<dyn std::error::Error>> {
        println!("ğŸ¯ å·¥ä½œæµå•å¸§æ£€æµ‹å¼€å§‹...");
        let start_time = Instant::now();
        
        // ç¡®ä¿alignment_systemå·²åˆå§‹åŒ–
        let mut alignment_sys = self.alignment_system.lock().unwrap();
        if alignment_sys.is_none() {
            return Err("åˆåƒæ£€æµ‹ç³»ç»Ÿæœªåˆå§‹åŒ–".into());
        }
        
        let sys = alignment_sys.as_mut().unwrap();
        
        // 1. æ‰§è¡Œåœ†å¿ƒæ£€æµ‹
        let (left_corners, right_corners) = sys.detect_circles_grid(
            &left_image,
            &right_image,
            "yaml_last_param_file/rectify_maps.yaml", // ğŸ”§ ä¿®æ­£è·¯å¾„
        )?;
        
        // 2. å·¦çœ¼å§¿æ€æ£€æµ‹
        let left_pose = sys.check_left_eye_pose(&left_corners)?;
        if !left_pose.pass {
            return Ok(DetectionResult::LeftEyePose {
                roll: left_pose.roll,
                pitch: left_pose.pitch,
                yaw: left_pose.yaw,
                pass: false,
                message: format!("âŒ å·¦çœ¼å§¿æ€è¶…å‡ºå®¹å·® - roll={:.3}Â°, pitch={:.3}Â°, yaw={:.3}Â°", 
                               left_pose.roll, left_pose.pitch, left_pose.yaw),
            });
        }
        
        // 3. å³çœ¼å§¿æ€æ£€æµ‹
        let right_pose = sys.check_right_eye_pose(&right_corners)?;
        if !right_pose.pass {
            return Ok(DetectionResult::RightEyePose {
                roll: right_pose.roll,
                pitch: right_pose.pitch,
                yaw: right_pose.yaw,
                pass: false,
                message: format!("âŒ å³çœ¼å§¿æ€è¶…å‡ºå®¹å·® - roll={:.3}Â°, pitch={:.3}Â°, yaw={:.3}Â°", 
                               right_pose.roll, right_pose.pitch, right_pose.yaw),
            });
        }
        
        // 4. åŒçœ¼åˆåƒæ£€æµ‹
        let alignment_result = sys.check_dual_eye_alignment(&left_corners, &right_corners, true)?;
        let adjustment_hint = format!(
            "è°ƒæ•´æç¤º: Î”x={:.3}px {}, Î”y={:.3}px {}",
            alignment_result.mean_dx,
            if alignment_result.mean_dx > 0.0 { "(å³çœ¼å‘å·¦è°ƒ)" } else { "(å³çœ¼å‘å³è°ƒ)" },
            alignment_result.mean_dy,
            if alignment_result.mean_dy < 0.0 { "(å³çœ¼å‘ä¸Šè°ƒ)" } else { "(å³çœ¼å‘ä¸‹è°ƒ)" }
        );
        
        let processing_time = start_time.elapsed();
        println!("âœ“ å·¥ä½œæµå•å¸§æ£€æµ‹å®Œæˆï¼Œæ€»è€—æ—¶: {:.1} ms", processing_time.as_millis());
        
        Ok(DetectionResult::DualEyeAlignment {
            mean_dx: alignment_result.mean_dx,
            mean_dy: alignment_result.mean_dy,
            rms: alignment_result.rms,
            p95: alignment_result.p95,
            max_err: alignment_result.max_err,
            pass: alignment_result.pass,
            adjustment_hint,
        })
    }
    
    /// ğŸ¯ ä»…æ‰§è¡Œåœ†å¿ƒæ£€æµ‹ - ç”¨äºå¿«é€ŸéªŒè¯å›¾åƒè´¨é‡
    pub fn detect_circles_only(
        &mut self,
        left_image: core::Mat,
        right_image: core::Mat,
    ) -> Result<(opencv::core::Vector<opencv::core::Point2f>, opencv::core::Vector<opencv::core::Point2f>), Box<dyn std::error::Error>> {
        let mut alignment_sys = self.alignment_system.lock().unwrap();
        if alignment_sys.is_none() {
            return Err("åˆåƒæ£€æµ‹ç³»ç»Ÿæœªåˆå§‹åŒ–".into());
        }
        
        let sys = alignment_sys.as_mut().unwrap();
        // ğŸ”§ ä¿®æ­£é‡æ˜ å°„çŸ©é˜µè·¯å¾„ - ä½¿ç”¨yaml_last_param_fileç›®å½•
        // æ—§è·¯å¾„: "rectify_maps.yaml"
        sys.detect_circles_grid(&left_image, &right_image, "yaml_last_param_file/rectify_maps.yaml")
    }
}

impl Drop for AlignmentWorkflow {
    fn drop(&mut self) {
        let _ = self.stop_workflow();
    }
}

// ==================== è¾…åŠ©å‡½æ•° ====================

/// å°†åŸå§‹å›¾åƒæ•°æ®è½¬æ¢ä¸ºBase64æ ¼å¼çš„PNGå›¾åƒ
fn raw_data_to_base64_image(raw_data: &[u8], width: i32, height: i32) -> Result<String, Box<dyn std::error::Error>> {
    use base64::{Engine as _, engine::general_purpose};
    use opencv::{core, imgcodecs, prelude::*};
    
    // å°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºOpenCV Mat
    let mat = AlignmentWorkflow::raw_data_to_mat(raw_data, width, height)?;
    
    // åˆ›å»ºç¼©ç•¥å›¾ (ç¼©æ”¾åˆ°400x300ä»¥å‡å°‘ä¼ è¾“æ•°æ®é‡)
    let thumbnail_width = 400;
    let thumbnail_height = (height as f32 * thumbnail_width as f32 / width as f32) as i32;
    
    let mut resized_mat = core::Mat::default();
    opencv::imgproc::resize(
        &mat,
        &mut resized_mat,
        core::Size::new(thumbnail_width, thumbnail_height),
        0.0,
        0.0,
        opencv::imgproc::INTER_LINEAR,
    )?;
    
    // è½¬æ¢ä¸ºPNGæ ¼å¼çš„å­—èŠ‚æ•°ç»„
    let mut buffer = opencv::core::Vector::<u8>::new();
    imgcodecs::imencode(".png", &resized_mat, &mut buffer, &opencv::core::Vector::new())?;
    
    // è½¬æ¢ä¸ºBase64
    let base64_data = general_purpose::STANDARD.encode(buffer.as_slice());
    Ok(format!("data:image/png;base64,{}", base64_data))
} 